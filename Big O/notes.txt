Big O Notation is a way to formalize fuzzy counting
It allows us to talk about how the runtime of an 
algorithm grows as the inputs grow

We say that an algorithm is O(f(n)) if the number
of simple operations is eventually less than a 
constant number of timesf(n), as n increases

f(n) could be linear (f(n) = n)
f(n) could be quadratic (f(n) = n2)
f(n) could be constant (f(n) = 1)
f(n) could also be something entirely different

When determining the time complexity of an algorithm 
there are some helpful rule of thumbs for expressions
these rules of thumb are consequences of Big O Notation

Constants don't matter:
O(2n) simplify to O(n)
O(500) simplify to O(1)
O(13n*n) simplify to O(n*n)

Smaller terms don't matter:
O(n+10) is O(n)
O(1000n + 50) is O(n)
O(n*n + 5n + 8) is O(n*n)

Big O Shorthands:
1.Arithmetic operations are constant
2.Variable assignment is constant
3.Accessing elements in an array(by index) or object(by key) is constant
4.In a loop, the complexity is the length of the loop times the 
  complexity of whatever happens inside the loop